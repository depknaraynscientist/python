{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Workshop on Machine Learning (Speaker : Deepak Narayan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note : Check if scikit-learn is available in your anaconda distribution, if not install it and then proceed.`\n",
    "\n",
    "scikit-learn library documentation : https://scikit-learn.org/  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas, seaborn, matplotlib\n",
    "import pandas as pd #pandas import\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt #matplotlib import\n",
    "%matplotlib inline \n",
    "#setting the plots as inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read from data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use a function to get the first few rows of the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many rows and columns are there in the dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run info() and check if there are any missing values for any columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run describe() and take a quick look at the output variable statistics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. Plot a KDE + histogram plot for the target variable 'medv' to look at its distribution.  \n",
    "b. What is the range around which we can see a high probability density, no need to give exact values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the dataset into 3 : training, validation and test sets.  \n",
    "For this,   \n",
    "1.Initially split it into two : training-validation with 80% of data and test with 20%.  \n",
    "2.Then split training-validation into two : training data 70% and validation data 30%.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "We will use the function `train_test_split()` from sklearn library for doing this.  \n",
    "First import the function alone by using :  `\n",
    "from sklearn.model_selection import train_test_split\n",
    "`\n",
    "Then as parameters to the function, provide the below :\n",
    "1. the input variable columns using .loc\n",
    "2. the output variable column\n",
    "3. test_size parameter (this is 0.2 for the first split that we will do, since the question asks for 20% test data. for the second split, it will be 0.3, since question asks for 30% validation data.\n",
    "4. shuffle (default is True. If you dont want to shuffle rows, then give False)\n",
    "5. random_state (Give this to get reproducible results when shuffle is True. You can give any number you want, but if you give the same number as the ones I use, then you will get similar results as mine.)\n",
    "6. Values returned are in the form x_...., x_...., y_...., y_....  .\n",
    "\n",
    "The values returned by the function are in the order dataset1_inputs, dataset2_inputs, dataset1_out, dataset2_out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the shape of the 6 dataframes obtained, namely the trainingdata_inpvariables, validationdata_inputvariables, testdata_inputvariables, trainingdata_outvariable, validationdata_outvariable, testdata_outvariable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape\n",
    "y_train.shape\n",
    "x_valid.shape\n",
    "y_valid.shape\n",
    "x_test.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Now that we have our training, validation and test data set, we can train our model for Simple and Multi-variate Linear Regression and check MSE, RMSE values. But how do we select the input variables? Selecting the input variables or 'Feature Selection' is an active research area.`  \n",
    "**Remember that `Wide Data is much more harder to work with than Long or Tall Data`. Feature selection becomes increasingly difficult with Wide Data whether it is for Linear Regression models or other models.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training/Building and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a Guideline for Supervised Learning Model Training and Testing, remember these steps. Irrespective of what Machine Learning method you use, the approach is similar.  \n",
    "\n",
    "For a single model, do the below :\n",
    "- Create a Linear Regression Object.\n",
    "- On that object, use .fit() to create a model.\n",
    "- On that object, use .predict() to predict on the data\n",
    "    - First predict on the training data. Get the MSE, RMSE. This is the training error.\n",
    "    - Predict on the validation data. Get the MSE, RMSE. This is the validation error.  \n",
    "\n",
    "Repeat the above steps for your next model.  \n",
    "\n",
    "We will look at these steps one by one below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the model on the training data using Linear Regression.  \n",
    "Lets first look at `Simple Linear Regression - output variable and 1 input variable`.  \n",
    "Consider the input variable as the lstat column.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1 : Create a Linear Regression object for further processing.**  \n",
    "quick sidenote: This is OOPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression #import the Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2 : Fit the model on the training data. Use .fit() to train the model.**  \n",
    "Important Note : syntax is *.fit(input columns, output column)*  \n",
    "Here we have only 1 predictor/input.  \n",
    "*.fit() will throw an error if we directly give a single column as a predictor. To workaround this, give the single predictor column as  df['colname'].to_numpy().reshape(-1,1). -1 indicates that python has to find out by itself how many rows should be present after the reshape. Else pass it as a list of list of items with only 1 item.*  \n",
    "\n",
    "Important note : For multi-variate Linear Regression, you can directly give the columns as predictors, no need to use ..to_numpy().reshape(-1,1) in that case. This needs to be only done when there is a single predictor.  \n",
    "*eg : linear_model.fit(x_train[['lstat', 'rm']], y_train)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model is now created! Find the coefficients of the Linear Regression model, that is beta0 and beta1.**  \n",
    "note : use .coef_ and .intercept_  \n",
    ".coef_ will return a numpy array of the coefficients starting from beta1.  \n",
    ".intercept_ will return beta0(also sometimes referred to as the constant coefficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets calculate MSE and RMSE on the training set.  \n",
    "Use .predict() for this.**  \n",
    "syntax is *.predict(input columns)*  \n",
    "Here we have only 1 predictor/input.  \n",
    "As before, *.predict() will throw an error if we directly give a single column as the input. To workaround this, give the single column as  df['colname'].values.reshape(-1,1). -1 indicates that python has to find out by itself how many rows should be present after the reshape.*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets take a moment to pause and look at this. The mse and rmse values are found on the training set. This rmse is called the training error.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our output variable is in 1000's of $. Similarly, the rmse is also in same units. So around ****$ error per predicted data point on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you change the input variable or predictor, the Linear Regression Model itself changes. A different input variable may increase or decrease the error. We have to try with different input variables and get a model which is most accurate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating the metrics on the training set is not enough. As the training set was used to actually create or train the model, we need to use a validation set for validating the metrics. Lets calculate MSE and RMSE on the validation set.**  \n",
    "note : follow similar procedure. Use .predict() on the validation set columns, thats it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consider the input variable as the 'rm' column. Create a Simple Linear Regression Model with medv as the output variable.  \n",
    "Calculate the MSE and RMSE on the training and validation sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do by self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets now look at `Multi-variate Linear Regression - output variable and multiple input variables`.**  \n",
    "**Create a Multi-variate Linear Regression Model. Consider the input variables as the LSTAT and TAX columns. Output variable is medv.  \n",
    "Calculate the MSE and RMSE on the training and validation sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Out of the three models we looked at, which model looks better to you based on validation set? Run your best model on the final test set and report the MSE and RMSE values. How does it perform?**  \n",
    "note : Your test set is the hold out set. It is used for checking how your model will perform in the real world scenario. Often the test set is carefully created to have different scenarios which happens in the real world. We are not doing that in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare between the three models validation set metrics and check which is better.\n",
    "#Use that model to predict on the test set and calculate the MSE and RMSE values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great Job!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
